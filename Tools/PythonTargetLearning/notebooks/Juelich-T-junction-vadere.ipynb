{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Search-trajectory-files-and-create-output-path\" data-toc-modified-id=\"Search-trajectory-files-and-create-output-path-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Search trajectory files and create output path</a></span></li><li><span><a href=\"#Plot-experiment-trajectories\" data-toc-modified-id=\"Plot-experiment-trajectories-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Plot experiment trajectories</a></span></li><li><span><a href=\"#Processing-Experiments\" data-toc-modified-id=\"Processing-Experiments-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Processing Experiments</a></span></li><li><span><a href=\"#Interactive-plot-of-density-images\" data-toc-modified-id=\"Interactive-plot-of-density-images-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Interactive plot of density images</a></span></li><li><span><a href=\"#Aggregate-unfiltered-density-distributions\" data-toc-modified-id=\"Aggregate-unfiltered-density-distributions-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Aggregate unfiltered density distributions</a></span></li><li><span><a href=\"#Filter-duplicated-images\" data-toc-modified-id=\"Filter-duplicated-images-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Filter duplicated images</a></span></li><li><span><a href=\"#Combine-images\" data-toc-modified-id=\"Combine-images-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Combine images</a></span></li><li><span><a href=\"#Aggregate-filtered-density-distributions\" data-toc-modified-id=\"Aggregate-filtered-density-distributions-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Aggregate filtered density distributions</a></span></li><li><span><a href=\"#Filter-50/50-distributions\" data-toc-modified-id=\"Filter-50/50-distributions-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Filter 50/50 distributions</a></span></li><li><span><a href=\"#Plot-density-maps-to-file\" data-toc-modified-id=\"Plot-density-maps-to-file-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Plot density maps to file</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T15:19:49.245933Z",
     "start_time": "2019-09-25T15:19:49.223407Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as matplot\n",
    "import itertools\n",
    "import math\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from os import listdir, mkdir, makedirs\n",
    "from os.path import join, isdir\n",
    "from ipywidgets import *\n",
    "from multiprocessing import Pool, Lock\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../python/python_src_preprocessing/target_learning'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils.reader.trajectory import get_all_trajectory_files, read_trajectory_file\n",
    "\n",
    "import preprocessing.preprocessing as processing\n",
    "import plotting.densitymap as plot\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport preprocessing.preprocessing, plotting.densitymap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search trajectory files and create output path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T09:14:09.746658Z",
     "start_time": "2019-09-24T09:14:09.741654Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# paramter\n",
    "input_path = '../datasets/trajectories/obstacle'\n",
    "output_path = 'check/240x200/csvs/obstacle'\n",
    "experiments = get_all_trajectory_files(input_path)\n",
    "#experiments = list(map(lambda x: join(input_path, x), listdir(input_path)))\n",
    "\n",
    "#print(experiments)\n",
    "\n",
    "if not isdir(output_path):\n",
    "    makedirs(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot experiment trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:11:53.555629Z",
     "start_time": "2019-08-22T13:11:53.548623Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "def toInch(pixel, dpi=92):\n",
    "    return pixel / dpi\n",
    "\n",
    "\n",
    "def plot_experiment(frame, ax):\n",
    "    # map p-ids to targets\n",
    "    for pId, group in frame.groupby('pedestrianId'):\n",
    "        data = group.loc[(group['x'] > 14.5) & (\n",
    "            group['x'] < 24.9) & (group['y'] < 8.7)]\n",
    "        ax.plot(data['x'], data['y'], zorder=1)\n",
    "\n",
    "    rect = matplot.Rectangle(\n",
    "        (18.49, 6.69), 2.4, 1, edgecolor='r', alpha=1, fill=False, zorder=2, linewidth=2)\n",
    "\n",
    "    ax.add_patch(rect)\n",
    "    ax.grid()\n",
    "    \n",
    "def common_start(sa, sb):\n",
    "    \"\"\" returns the longest common substring from the beginning of sa and sb \n",
    "    taken from https://stackoverflow.com/questions/18715688/find-common-substring-between-two-strings\n",
    "    \"\"\"\n",
    "    def _iter():\n",
    "        for a, b in zip(sa, sb):\n",
    "            if a == b:\n",
    "                yield a\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    return ''.join(_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot all scenario trajectories to file\n",
    "# %%capture\n",
    "plots = len(experiments)\n",
    "rows = math.ceil(plots / 2.0)\n",
    "fig, ax = matplot.subplots(nrows=rows, ncols=2, figsize=(\n",
    "    toInch(1000), toInch(rows * 300)), dpi=92)\n",
    "\n",
    "ax = np.array(ax).flatten()\n",
    "\n",
    "for idx in range(plots):\n",
    "    experiment = experiments[idx]\n",
    "    directory = os.path.dirname(experiment)\n",
    "    name = os.path.basename(directory) #.split('_')[0]\n",
    "\n",
    "    ax[idx].set_title(name)\n",
    "\n",
    "    frame = read_trajectory_file(experiment)\n",
    "    plot_experiment(frame, ax[idx])\n",
    "\n",
    "fig.subplots_adjust(hspace=0.25)\n",
    "#fig.savefig('vadere-scenario-trajectories.pdf', transparent=True)\n",
    "matplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T14:46:10.556336Z",
     "start_time": "2019-09-20T14:28:46.634910Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# processing experiments\n",
    "\n",
    "results = []\n",
    "for experiment in tqdm_notebook(experiments, desc='process files'):\n",
    "    directory = os.path.dirname(experiment)\n",
    "    name = os.path.basename(directory)\n",
    "    \n",
    "    if os.path.exists(join(output_path, name + '.csv')):\n",
    "        print('skipping', name)\n",
    "        continue\n",
    "\n",
    "    frame = None # read_trajectory_file(experiment)\n",
    "    pId2Target = {r['pedestrianId']: r['sourceId'] for i, r in pd.read_csv(\n",
    "        join(directory, 'sources.csv'), sep=' ', skiprows=1, header=None, names=['pedestrianId', 'sourceId']).iterrows()}\n",
    "    \n",
    "    context = dict({\n",
    "        'name': name,\n",
    "        'targets': [1, 2],\n",
    "        'area':  [18.50, 5.7, 2.4, 2],\n",
    "        'exact': True,\n",
    "        'sigma': 0.7,\n",
    "        'resolution': 0.1,\n",
    "        'pedestrian_radius': 0.195,\n",
    "        'gauss_bounds': 2,\n",
    "        'pId2Target': pId2Target,\n",
    "        'processors': [\n",
    "            processing.process_densities,\n",
    "            processing.process_pedestrians,\n",
    "            processing.process_percentiles\n",
    "        ],\n",
    "        'number_of_cores': 5,\n",
    "        'skip': 1,\n",
    "        'timesteps': join(directory, 'timesteps')\n",
    "    })\n",
    "    context = processing.process_experiment(frame, context)\n",
    "    results.append(context)\n",
    "\n",
    "    densities = np.array(context.get('densities'))\n",
    "    # sort by timestep number\n",
    "    densities = densities[densities[:,0].argsort()]\n",
    "    with open(join(output_path, name + '.csv'), mode='w') as file:\n",
    "        for density in densities:\n",
    "            file.write(\n",
    "                ';'.join(map(lambda r: \"{:.10f}\".format(r), density[1:])) + '\\n')\n",
    "\n",
    "        file.flush()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive plot of density images\n",
    "<font color='red' size='5'><b>Important note:</b><br/>This requires that the result of the processed files (cell above) is still available</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     3,
     7
    ]
   },
   "outputs": [],
   "source": [
    "# interactive density maps\n",
    "%matplotlib notebook\n",
    "\n",
    "def extrac_xy(pedestrians, x0, y0, resolution):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for _, ped in pedestrians.iterrows():\n",
    "        x.append(int(abs(ped['x'] - x0) * (1 / resolution)))\n",
    "        y.append(int(abs(ped['y'] - y0) * (1 / resolution)))\n",
    "\n",
    "    return x, y\n",
    "\n",
    "for result in results:\n",
    "    name = result.get('name')\n",
    "    densities = result.get('densities')\n",
    "    area = result.get('area')\n",
    "    peds = result.get('pedestrians')\n",
    "    size = result.get('size')\n",
    "    res = result.get('resolution')\n",
    "    t = len(result.get('targets'))\n",
    "\n",
    "    start = 0\n",
    "\n",
    "    first = densities[start][:-t].reshape(size)\n",
    "\n",
    "    fig, ax = matplot.subplots()\n",
    "\n",
    "    cbar = None\n",
    "\n",
    "    ax.set_xticks(np.linspace(0, len(first[0]) - 1, 13))\n",
    "    ax.set_xticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2,\n",
    "                        1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "\n",
    "    ax.set_yticks(np.linspace(0, len(first) - 1, 6))\n",
    "    ax.set_yticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "    def update(frame):\n",
    "        global cbar\n",
    "        f = densities[frame][:-t].reshape(size)\n",
    "\n",
    "        ax.clear()\n",
    "        if cbar is not None:\n",
    "            cbar.remove()\n",
    "\n",
    "        img = ax.imshow(f, interpolation='Nearest')\n",
    "        cbar = fig.colorbar(img)\n",
    "\n",
    "        x, y = extrac_xy(peds[frame], area[0], area[1], res)\n",
    "\n",
    "        ax.plot(x, y, '.r', markersize=10)\n",
    "\n",
    "        ax.set_xticks(np.linspace(0, len(first[0]) - 1, 13))\n",
    "        ax.set_xticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0,\n",
    "                            1.2, 1.4, 1.6, 1.8, 2.0, 2.2, 2.4])\n",
    "\n",
    "        ax.set_yticks(np.linspace(0, len(first) - 1, 6))\n",
    "        ax.set_yticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "\n",
    "    interact(update, frame=widgets.IntSlider(\n",
    "        min=0, max=len(densities)-1, step=1, value=start))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate unfiltered density distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-22T13:03:11.203522Z",
     "start_time": "2019-08-22T13:03:10.368231Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# aggregate distributions\n",
    "\n",
    "# output_p = 'juelich-vadere/output_01_save/'\n",
    "outputs = list(filter(lambda o: os.path.isfile(os.path.join(output_path, o)) and 'filtered' not in o, os.listdir(output_path)))\n",
    "dfs = []\n",
    "for output in tqdm_notebook(outputs, desc=\"Loading density maps\", total=len(outputs)):\n",
    "    dfs.append(pd.read_csv(os.path.join(\n",
    "        output_path, output), sep=';', header=None))\n",
    "\n",
    "frame = pd.concat([df.iloc[:, -2:] for df in dfs], sort=False)\n",
    "print(frame.shape)\n",
    "display(frame.groupby(list(frame.columns), as_index=False).size().to_frame(\n",
    "    'frames').reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter duplicated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T14:51:06.633594Z",
     "start_time": "2019-09-20T14:51:05.399906Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter consecutive duplicates\n",
    "\n",
    "outputs = list(filter(lambda o: os.path.isfile(os.path.join(output_path, o)), os.listdir(output_path)))\n",
    "\n",
    "if not isdir(os.path.join(output_path, 'filtered')):\n",
    "    makedirs(os.path.join(output_path, 'filtered'))\n",
    "\n",
    "for output in tqdm_notebook(outputs, desc=\"filter density maps\"):\n",
    "    last = None\n",
    "    skipped = 0\n",
    "    written = 0\n",
    "    with open(os.path.join(output_path, output), \"r\") as full:\n",
    "        \n",
    "        with open(os.path.join(output_path, 'filtered', output), \"w\") as filtered:\n",
    "            for line in full:\n",
    "                if line != last:\n",
    "                    filtered.write(line)\n",
    "                    last = line\n",
    "                    written = written + 1\n",
    "                    continue\n",
    "\n",
    "                skipped = skipped + 1\n",
    "    print(output, written, 'lines written,', skipped, 'lines skipped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine images of same scenario types\n",
    "\n",
    "# navigation\n",
    "outputs = list(filter(lambda o: 'filtered' in o and 'navigation' in o, os.listdir(os.path.join(output_path, 'navigation'))))\n",
    "pd.concat( [ pd.read_csv(os.path.join(output_path, f)) for f in outputs ], sort=False ).to_csv( os.path.join(output_path, 'navigation', 'combined.csv'), index=False)\n",
    "\n",
    "# unit\n",
    "outputs = list(filter(lambda o: 'filtered' in o and 'unit' in o, os.listdir(os.path.join(output_path, 'unit'))))\n",
    "pd.concat( [ pd.read_csv(os.path.join(output_path, f)) for f in outputs ], sort=False ).to_csv( os.path.join(output_path, 'unit', 'combined.csv'), index=False)\n",
    "\n",
    "# obstacle weight 0.3\n",
    "outputs = list(filter(lambda o: ('filtered' in o) and ('navigation' not in o) and ('unit' not in o), os.listdir(os.path.join(output_path, 'obstacle'))))\n",
    "pd.concat( [ pd.read_csv(os.path.join(output_path, f)) for f in outputs ], sort=False ).to_csv( os.path.join(output_path,'obstacle', 'combined.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate filtered density distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T15:13:28.285309Z",
     "start_time": "2019-09-20T15:13:26.033262Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# aggregate distributions\n",
    "\n",
    "# output_p = 'juelich-vadere/output_01_save/'\n",
    "# for t in ['navigation', 'unit', 'obstacle']:\n",
    "outputs = os.listdir(os.path.join(output_path, 'filtered'))\n",
    "dfs = []\n",
    "for output in tqdm_notebook(outputs, desc=\"Loading density maps\", total=len(outputs)):\n",
    "    dfs.append(pd.read_csv(os.path.join(\n",
    "        output_path, output), sep=';', header=None))\n",
    "\n",
    "frame = pd.concat([df.iloc[:, -2:] for df in dfs], sort=False)\n",
    "print(frame.shape)\n",
    "frame = frame.groupby(list(frame.columns), as_index=False).size().to_frame(\n",
    "    'frames').reset_index()\n",
    "\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter 50/50 distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T15:20:34.549171Z",
     "start_time": "2019-09-25T15:20:32.461188Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# filter every second 50% 50% distribution\n",
    "output_path = 'check/240x200/csvs/obstacle'\n",
    "outputs = os.listdir(os.path.join(output_path, 'filtered'))\n",
    "\n",
    "if not isdir(os.path.join(output_path, 'reduced')):\n",
    "    makedirs(os.path.join(output_path, 'reduced'))\n",
    "\n",
    "for output in tqdm_notebook(outputs, desc=\"filter density distributions\"):\n",
    "    skip = True\n",
    "    skipped = 0\n",
    "    written = 0\n",
    "    with open(os.path.join(output_path, 'filtered', output), \"r\") as full:\n",
    "        with open(os.path.join(output_path, 'reduced', output), \"w\") as filtered:\n",
    "            for line in full:\n",
    "                if line.strip().endswith('0.5000000000;0.5000000000'):\n",
    "                    skip = not(skip)\n",
    "                    \n",
    "                    if skip:\n",
    "                        skipped = skipped + 1\n",
    "                        continue\n",
    "                \n",
    "                filtered.write(line)\n",
    "                written = written + 1\n",
    "\n",
    "    print(written, 'lines written,', skipped, 'lines skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T17:10:40.439679Z",
     "start_time": "2019-09-25T17:10:39.738527Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_path = 'check/240x100/csvs/obstacle'    \n",
    "# aggregate distributions\n",
    "outputs = os.listdir(os.path.join(output_path, 'reduced'))\n",
    "dfs = []\n",
    "for output in outputs:\n",
    "    dfs.append(pd.read_csv(os.path.join(\n",
    "        output_path, 'reduced', output), sep=';', header=None))\n",
    "\n",
    "frame = pd.concat([df.iloc[:, -2:] for df in dfs], sort=False)\n",
    "print(frame.shape)\n",
    "frame = frame.groupby(list(frame.columns), as_index=False).size().to_frame(\n",
    "    'frames').reset_index()\n",
    "\n",
    "print(frame.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot density maps to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T17:10:31.011500Z",
     "start_time": "2019-09-25T17:10:23.378891Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save density maps\n",
    "\n",
    "image_directory = \"check/240x100/images\"\n",
    "\n",
    "if not os.path.isdir(image_directory):\n",
    "    os.mkdir(image_directory)\n",
    "\n",
    "def filter_chunk(chunk):\n",
    "    return chunk.loc[(chunk[231] == 0.5) & (chunk[232] == 0.5)]\n",
    "    \n",
    "outputs = os.listdir(os.path.join(output_path, 'reduced'))\n",
    "for output in tqdm_notebook(outputs):\n",
    "    plot.processFile(output, os.path.join(output_path, 'reduced'), image_directory, (20, 24)) # , _filter=filter_chunk\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
